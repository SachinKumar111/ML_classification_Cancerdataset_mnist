{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Classification_logisticReg.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYHWlDO6mlMM",
        "colab_type": "text"
      },
      "source": [
        "# Binary Classification\n",
        "# Data Munging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXYo1nHqmlMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "data_train= pd.read_csv(\"data/wisconsin_data/train_wbcd.csv\")\n",
        "data_test= pd.read_csv(\"data/wisconsin_data/test_wbcd.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ3qpzngmlMe",
        "colab_type": "code",
        "colab": {},
        "outputId": "b83102e9-d24a-4b60-f1f7-f8ed3e7c13e1"
      },
      "source": [
        "features_data_test=data_test.iloc[:, 2:]\n",
        "print(\" No. of Features:\", len(features_data_test.columns))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " No. of Features: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIUmDNgBmlMi",
        "colab_type": "code",
        "colab": {},
        "outputId": "43b91789-d045-4cd0-cd0f-ec67c0e51e37"
      },
      "source": [
        "data_test['Diagnosis'].value_counts()\n",
        "#B's and M's are not balanced as the distribution is different. Number of M is only 6 which is only 30% of the data and number of B is 14 which is 70% of data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    14\n",
              "M     6\n",
              "Name: Diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0g4dOpNmlMr",
        "colab_type": "code",
        "colab": {},
        "outputId": "5188f5af-65f7-480f-e303-583079367d24"
      },
      "source": [
        "data_train['Diagnosis'].value_counts()\n",
        "#B's and M's are not balanced as the distribution are slightly different.42% of B's are in the data whereas 58% of M's are in the data."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "B    58\n",
              "M    42\n",
              "Name: Diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bltZA7yBmlMz",
        "colab_type": "code",
        "colab": {},
        "outputId": "d1c5983f-9ab8-44a6-e6ea-1c28ba41de4b"
      },
      "source": [
        "data_train= data_train.replace(0, np.NAN)\n",
        "data_train= data_train.replace('', np.NAN)\n",
        "data_train.isna().sum()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Patient_ID    0\n",
              "Diagnosis     0\n",
              "f1            0\n",
              "f2            0\n",
              "f3            0\n",
              "f4            0\n",
              "f5            0\n",
              "f6            0\n",
              "f7            6\n",
              "f8            6\n",
              "f9            0\n",
              "f10           0\n",
              "f11           0\n",
              "f12           0\n",
              "f13           0\n",
              "f14           0\n",
              "f15           0\n",
              "f16           0\n",
              "f17           6\n",
              "f18           6\n",
              "f19           0\n",
              "f20           0\n",
              "f21           2\n",
              "f22           0\n",
              "f23           0\n",
              "f24           0\n",
              "f25           0\n",
              "f26           0\n",
              "f27           6\n",
              "f28           6\n",
              "f29           0\n",
              "f30           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQdn4J8mlM9",
        "colab_type": "code",
        "colab": {},
        "outputId": "9603f01f-d6aa-4545-d1aa-eb31eff3e779"
      },
      "source": [
        "data_test= data_test.replace(0, np.NAN)\n",
        "data_test= data_test.replace('', np.NAN)\n",
        "missing=pd.DataFrame(data_test.isna().sum())\n",
        "print(\"Number of missing in test data:\", len(missing[missing[0]!=0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing in test data: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7pfgBjmlNA",
        "colab_type": "code",
        "colab": {},
        "outputId": "dc3fb447-cdfb-4f42-e445-7f76cc2a9ca5"
      },
      "source": [
        "data_train= data_train.replace(0, np.NAN)\n",
        "data_train= data_train.replace('', np.NAN)\n",
        "missing=pd.DataFrame(data_train.isna().sum())\n",
        "print(\"Number of missing in train data:\", len(missing[missing[0]!=0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of missing in train data: 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgpJoLhqmlNF",
        "colab_type": "code",
        "colab": {},
        "outputId": "6c3ec566-cebb-4c24-bbb4-39d88c7da0ce"
      },
      "source": [
        "data_train=data_train.fillna(data_train.median())\n",
        "print(data_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Patient_ID Diagnosis      f1     f2      f3      f4       f5       f6  \\\n",
            "0       909410         B  14.020  15.66   89.59   606.5  0.07966  0.05581   \n",
            "1     84358402         M  20.290  14.34  135.10  1297.0  0.10030  0.13280   \n",
            "2      8912284         B  12.890  15.70   84.08   516.6  0.07818  0.09580   \n",
            "3     90317302         B  10.260  12.22   65.75   321.6  0.09996  0.07542   \n",
            "4       914102         B  13.160  20.54   84.06   538.7  0.07335  0.05275   \n",
            "5       924342         B   9.333  21.94   59.01   264.0  0.09240  0.05605   \n",
            "6      8911164         B  11.890  17.36   76.20   435.6  0.12250  0.07210   \n",
            "7       893548         B  13.050  13.84   82.71   530.6  0.08352  0.03735   \n",
            "8       867739         M  18.450  21.91  120.20  1075.0  0.09430  0.09709   \n",
            "9       857374         B  11.940  18.24   75.71   437.6  0.08261  0.04751   \n",
            "10      853201         M  17.570  15.05  115.00   955.1  0.09847  0.11570   \n",
            "11      857373         B  13.640  16.34   87.21   571.8  0.07685  0.06059   \n",
            "12       86561         B  13.850  17.21   88.44   588.7  0.08785  0.06136   \n",
            "13      899147         B  11.950  14.96   77.23   426.7  0.11580  0.12060   \n",
            "14      859464         B   9.465  21.01   60.11   269.4  0.10440  0.07773   \n",
            "15     8911800         B  13.590  17.84   86.24   572.3  0.07948  0.04052   \n",
            "16      871641         B  11.080  14.71   70.21   372.7  0.10060  0.05743   \n",
            "17      922577         B  10.320  16.35   65.31   324.9  0.09434  0.04994   \n",
            "18      897604         B  12.990  14.23   84.08   514.3  0.09462  0.09965   \n",
            "19    88119002         M  19.530  32.47  128.00  1223.0  0.08420  0.11300   \n",
            "20      925277         B  14.590  22.68   96.39   657.1  0.08473  0.13300   \n",
            "21     8810436         B  15.270  12.91   98.17   725.5  0.08182  0.06230   \n",
            "22      908489         M  13.980  19.62   91.12   599.5  0.10600  0.11330   \n",
            "23      873701         M  15.700  20.31  101.20   766.6  0.09597  0.08799   \n",
            "24      915940         B  14.580  13.66   94.29   658.8  0.09832  0.08918   \n",
            "25      869476         B  11.900  14.65   78.11   432.8  0.11520  0.12960   \n",
            "26      906539         B  11.570  19.04   74.20   409.7  0.08546  0.07722   \n",
            "27      862548         M  14.420  19.77   94.48   642.5  0.09752  0.11410   \n",
            "28      868999         B   9.738  11.97   61.24   288.5  0.09250  0.04102   \n",
            "29      859196         B   9.173  13.86   59.20   260.9  0.07721  0.08751   \n",
            "..         ...       ...     ...    ...     ...     ...      ...      ...   \n",
            "70      922576         B  13.620  23.23   87.19   573.2  0.09246  0.06747   \n",
            "71      858981         B   8.598  20.98   54.66   221.8  0.12430  0.08963   \n",
            "72      862717         M  13.610  24.98   88.05   582.7  0.09488  0.08511   \n",
            "73      864496         B   8.726  15.83   55.84   230.9  0.11500  0.08201   \n",
            "74      843786         M  12.450  15.70   82.57   477.1  0.12780  0.17000   \n",
            "75      911916         M  16.250  19.51  109.80   815.8  0.10260  0.18930   \n",
            "76     9010259         B  13.050  18.59   85.09   512.0  0.10820  0.13040   \n",
            "77      921092         B   7.729  25.49   47.98   178.8  0.08098  0.04878   \n",
            "78      894089         B  12.490  16.85   79.19   481.6  0.08511  0.03834   \n",
            "79      854268         M  14.250  21.72   93.63   633.0  0.09823  0.10980   \n",
            "80      844981         M  13.000  21.82   87.50   519.8  0.12730  0.19320   \n",
            "81      875099         B   9.720  18.22   60.73   288.1  0.06950  0.02344   \n",
            "82     9113846         B  12.270  29.97   77.42   465.4  0.07699  0.03398   \n",
            "83       86355         M  22.270  19.67  152.80  1509.0  0.13260  0.27680   \n",
            "84      923169         B   9.683  19.34   61.05   285.7  0.08491  0.05030   \n",
            "85      855625         M  19.070  24.81  128.30  1104.0  0.09081  0.21900   \n",
            "86      912600         B  15.730  11.28  102.80   747.2  0.10430  0.12990   \n",
            "87      859575         M  18.940  21.31  123.60  1130.0  0.09009  0.10290   \n",
            "88    86730502         M  16.160  21.54  106.20   809.8  0.10080  0.12840   \n",
            "89      909445         M  17.270  25.42  112.40   928.8  0.08331  0.11090   \n",
            "90      855138         M  13.480  20.82   88.40   559.2  0.10160  0.12550   \n",
            "91        8670         M  15.460  19.48  101.70   748.9  0.10920  0.12230   \n",
            "92      925236         B   9.423  27.88   59.26   271.3  0.08123  0.04971   \n",
            "93      901288         M  20.640  17.35  134.80  1335.0  0.09446  0.10760   \n",
            "94      854002         M  19.270  26.47  127.90  1162.0  0.09401  0.17190   \n",
            "95     8611555         M  25.220  24.91  171.50  1878.0  0.10630  0.26650   \n",
            "96      873593         M  21.090  26.57  142.70  1311.0  0.11410  0.28320   \n",
            "97      891703         B  11.850  17.46   75.54   432.7  0.08372  0.05642   \n",
            "98      925311         B  11.200  29.37   70.67   386.0  0.07449  0.03558   \n",
            "99       89813         B  14.420  16.54   94.15   641.2  0.09751  0.11390   \n",
            "\n",
            "          f7        f8  ...     f21    f22     f23     f24      f25      f26  \\\n",
            "0   0.020870  0.026520  ...  14.910  19.31   96.53   688.9  0.10340  0.10170   \n",
            "1   0.198000  0.104300  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500   \n",
            "2   0.111500  0.033900  ...  13.900  19.69   92.12   595.6  0.09926  0.23170   \n",
            "3   0.019230  0.019680  ...  11.380  15.65   73.23   394.5  0.13430  0.16500   \n",
            "4   0.018000  0.012560  ...  14.500  28.46   95.29   648.3  0.11180  0.16460   \n",
            "5   0.039960  0.012820  ...   9.845  25.05   62.86   295.8  0.11030  0.08298   \n",
            "6   0.059290  0.074040  ...  12.400  18.99   79.46   472.4  0.13590  0.08368   \n",
            "7   0.004559  0.008829  ...  14.730  17.40   93.96   672.4  0.10160  0.05847   \n",
            "8   0.115300  0.068470  ...  22.520  31.39  145.60  1590.0  0.14650  0.22750   \n",
            "9   0.019720  0.013490  ...  13.100  21.33   83.67   527.2  0.11440  0.08906   \n",
            "10  0.098750  0.079530  ...  20.010  19.52  134.90  1227.0  0.12550  0.28120   \n",
            "11  0.018570  0.017230  ...  14.670  23.19   96.08   656.7  0.10890  0.15820   \n",
            "12  0.014200  0.011410  ...  15.490  23.58  100.30   725.9  0.11570  0.13500   \n",
            "13  0.011710  0.017870  ...  12.810  17.72   83.09   496.2  0.12930  0.18850   \n",
            "14  0.021720  0.015040  ...  10.410  31.56   67.03   330.7  0.15480  0.16640   \n",
            "15  0.019970  0.012380  ...  15.500  26.10   98.91   739.1  0.10500  0.07622   \n",
            "16  0.023630  0.025830  ...  11.350  16.82   72.01   396.5  0.12160  0.08240   \n",
            "17  0.010120  0.005495  ...  11.250  21.77   71.12   384.9  0.12850  0.08842   \n",
            "18  0.037380  0.020980  ...  13.720  16.91   87.38   576.0  0.11420  0.19750   \n",
            "19  0.114500  0.066370  ...  27.900  45.41  180.20  2477.0  0.14080  0.40970   \n",
            "20  0.102900  0.037360  ...  15.480  27.27  105.90   733.5  0.10260  0.31710   \n",
            "21  0.058920  0.031570  ...  17.380  15.92  113.70   932.7  0.12220  0.21860   \n",
            "22  0.112600  0.064630  ...  17.040  30.80  113.90   869.3  0.16130  0.35680   \n",
            "23  0.065930  0.051890  ...  20.110  32.82  129.30  1269.0  0.14140  0.35470   \n",
            "24  0.082220  0.043490  ...  16.760  17.24  108.50   862.0  0.12230  0.19280   \n",
            "25  0.037100  0.030030  ...  13.150  16.51   86.26   509.6  0.14240  0.25170   \n",
            "26  0.054850  0.014280  ...  13.070  26.98   86.43   520.5  0.12490  0.19370   \n",
            "27  0.093880  0.058390  ...  16.330  30.86  109.50   826.4  0.14310  0.30260   \n",
            "28  0.071945  0.036320  ...  10.620  14.10   66.53   342.9  0.12340  0.07204   \n",
            "29  0.059880  0.021800  ...  10.010  19.23   65.59   310.1  0.09836  0.16780   \n",
            "..       ...       ...  ...     ...    ...     ...     ...      ...      ...   \n",
            "70  0.029740  0.024430  ...  15.315  29.09   97.58   729.8  0.12160  0.15170   \n",
            "71  0.030000  0.009259  ...   9.565  27.04   62.06   273.9  0.16390  0.16980   \n",
            "72  0.086250  0.044890  ...  16.990  35.27  108.60   906.5  0.12650  0.19430   \n",
            "73  0.041320  0.019240  ...   9.628  19.62   64.48   284.4  0.17240  0.23640   \n",
            "74  0.157800  0.080890  ...  15.315  23.75  103.40   741.6  0.17910  0.52490   \n",
            "75  0.223600  0.091940  ...  17.390  23.05  122.10   939.7  0.13770  0.44620   \n",
            "76  0.096030  0.056030  ...  14.190  24.85   94.22   591.2  0.13430  0.26580   \n",
            "77  0.071945  0.036320  ...   9.077  30.92   57.17   248.0  0.12560  0.08340   \n",
            "78  0.004473  0.006423  ...  13.340  19.71   84.48   544.2  0.11040  0.04953   \n",
            "79  0.131900  0.055980  ...  15.890  30.36  116.20   799.6  0.14460  0.42380   \n",
            "80  0.185900  0.093530  ...  15.490  30.73  106.20   739.3  0.17030  0.54010   \n",
            "81  0.071945  0.036320  ...   9.968  20.83   62.25   303.8  0.07117  0.02729   \n",
            "82  0.071945  0.036320  ...  13.450  38.05   85.08   558.9  0.09422  0.05213   \n",
            "83  0.426400  0.182300  ...  28.400  28.01  206.80  2360.0  0.17010  0.69970   \n",
            "84  0.023370  0.009615  ...  10.930  25.59   69.10   364.2  0.11990  0.09546   \n",
            "85  0.210700  0.099610  ...  24.090  33.17  177.40  1651.0  0.12470  0.74440   \n",
            "86  0.119100  0.062110  ...  17.010  14.20  112.50   854.3  0.15410  0.29790   \n",
            "87  0.108000  0.079510  ...  24.860  26.58  165.90  1866.0  0.11930  0.23360   \n",
            "88  0.104300  0.056130  ...  19.470  31.68  129.70  1175.0  0.13950  0.30550   \n",
            "89  0.120400  0.057360  ...  20.380  35.46  132.80  1284.0  0.14360  0.41220   \n",
            "90  0.106300  0.054390  ...  15.530  26.02  107.30   740.4  0.16100  0.42250   \n",
            "91  0.146600  0.080870  ...  19.260  26.00  124.90  1156.0  0.15460  0.23940   \n",
            "92  0.071945  0.036320  ...  10.490  34.24   66.50   330.6  0.10730  0.07158   \n",
            "93  0.152700  0.089410  ...  25.370  23.17  166.80  1946.0  0.15620  0.30550   \n",
            "94  0.165700  0.075930  ...  24.150  30.90  161.40  1813.0  0.15090  0.65900   \n",
            "95  0.333900  0.184500  ...  30.000  33.62  211.70  2562.0  0.15730  0.60760   \n",
            "96  0.248700  0.149600  ...  26.680  33.48  176.50  2089.0  0.14910  0.75840   \n",
            "97  0.026880  0.022800  ...  13.060  25.75   84.35   517.8  0.13690  0.17580   \n",
            "98  0.071945  0.036320  ...  11.920  38.30   75.19   439.6  0.09267  0.05494   \n",
            "99  0.080070  0.042230  ...  16.670  21.51  111.40   862.1  0.12940  0.33710   \n",
            "\n",
            "        f27      f28     f29      f30  \n",
            "0   0.06260  0.08216  0.2136  0.06710  \n",
            "1   0.40000  0.16250  0.2364  0.07678  \n",
            "2   0.33440  0.10170  0.1999  0.07127  \n",
            "3   0.08615  0.06696  0.2937  0.07722  \n",
            "4   0.07698  0.04195  0.2687  0.07429  \n",
            "5   0.07993  0.02564  0.2435  0.07393  \n",
            "6   0.07153  0.08946  0.2220  0.06033  \n",
            "7   0.01824  0.03532  0.2107  0.06580  \n",
            "8   0.39650  0.13790  0.3109  0.07610  \n",
            "9   0.09203  0.06296  0.2785  0.07408  \n",
            "10  0.24890  0.14560  0.2756  0.07919  \n",
            "11  0.10500  0.08586  0.2346  0.08025  \n",
            "12  0.08115  0.05104  0.2364  0.07182  \n",
            "13  0.03122  0.04766  0.3124  0.07590  \n",
            "14  0.09412  0.06517  0.2878  0.09211  \n",
            "15  0.10600  0.05185  0.2335  0.06263  \n",
            "16  0.03938  0.04306  0.1902  0.07313  \n",
            "17  0.04384  0.02381  0.2681  0.07399  \n",
            "18  0.14500  0.05850  0.2432  0.10090  \n",
            "19  0.39950  0.16250  0.2713  0.07568  \n",
            "20  0.36620  0.11050  0.2258  0.08004  \n",
            "21  0.29620  0.10350  0.2320  0.07474  \n",
            "22  0.40690  0.18270  0.3179  0.10550  \n",
            "23  0.29020  0.15410  0.3437  0.08631  \n",
            "24  0.24920  0.09186  0.2626  0.07048  \n",
            "25  0.09420  0.06042  0.2727  0.10360  \n",
            "26  0.25600  0.06664  0.3035  0.08284  \n",
            "27  0.31940  0.15650  0.2718  0.09353  \n",
            "28  0.28540  0.11610  0.3105  0.08151  \n",
            "29  0.13970  0.05087  0.3282  0.08490  \n",
            "..      ...      ...     ...      ...  \n",
            "70  0.10490  0.07174  0.2642  0.06953  \n",
            "71  0.09001  0.02778  0.2972  0.07712  \n",
            "72  0.31690  0.11840  0.2651  0.07397  \n",
            "73  0.24560  0.10500  0.2926  0.10170  \n",
            "74  0.53550  0.17410  0.3985  0.12440  \n",
            "75  0.58970  0.17750  0.3318  0.09136  \n",
            "76  0.25730  0.12580  0.3113  0.08317  \n",
            "77  0.28540  0.11610  0.3058  0.09938  \n",
            "78  0.01938  0.02784  0.1917  0.06174  \n",
            "79  0.51860  0.14470  0.3591  0.10140  \n",
            "80  0.53900  0.20600  0.4378  0.10720  \n",
            "81  0.28540  0.11610  0.1909  0.06559  \n",
            "82  0.28540  0.11610  0.2409  0.06743  \n",
            "83  0.96080  0.29100  0.4055  0.09789  \n",
            "84  0.09350  0.03846  0.2552  0.07920  \n",
            "85  0.72420  0.24930  0.4670  0.10380  \n",
            "86  0.40040  0.14520  0.2557  0.08181  \n",
            "87  0.26870  0.17890  0.2551  0.06589  \n",
            "88  0.29920  0.13120  0.3480  0.07619  \n",
            "89  0.50360  0.17390  0.2500  0.07944  \n",
            "90  0.50300  0.22580  0.2807  0.10710  \n",
            "91  0.37910  0.15140  0.2837  0.08019  \n",
            "92  0.28540  0.11610  0.2475  0.06969  \n",
            "93  0.41590  0.21120  0.2689  0.07055  \n",
            "94  0.60910  0.17850  0.3672  0.11230  \n",
            "95  0.64760  0.28670  0.2355  0.10510  \n",
            "96  0.67800  0.29030  0.4098  0.12840  \n",
            "97  0.13160  0.09140  0.3101  0.07007  \n",
            "98  0.28540  0.11610  0.1566  0.05905  \n",
            "99  0.37550  0.14140  0.3053  0.08764  \n",
            "\n",
            "[100 rows x 32 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chB9m_lqmlNP",
        "colab_type": "code",
        "colab": {},
        "outputId": "3025c898-8a01-4ee6-e6a5-b22df77b7245"
      },
      "source": [
        "data_test=data_test.fillna(data_test.median())\n",
        "print(data_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Patient_ID Diagnosis      f1     f2      f3      f4       f5       f6  \\\n",
            "0       894047         B   8.597  18.60   54.09   221.2  0.10740  0.05847   \n",
            "1       892189         M  11.760  18.14   75.00   431.1  0.09968  0.05914   \n",
            "2      8810528         B  11.840  18.94   75.51   428.0  0.08871  0.06900   \n",
            "3       905978         B   9.405  21.70   59.60   271.2  0.10440  0.06159   \n",
            "4    871001502         B   8.219  20.70   53.27   203.9  0.09405  0.13050   \n",
            "5        87880         M  13.810  23.75   91.56   597.8  0.13230  0.17680   \n",
            "6       882488         B   9.567  15.91   60.21   279.6  0.08464  0.04087   \n",
            "7    911296202         M  27.420  26.27  186.90  2501.0  0.10840  0.19880   \n",
            "8       861648         B  14.620  24.02   94.57   662.7  0.08974  0.08606   \n",
            "9       895100         M  20.340  21.51  135.90  1264.0  0.11700  0.18750   \n",
            "10      853612         M  11.840  18.70   77.93   440.6  0.11090  0.15160   \n",
            "11     8510653         B  13.080  15.71   85.63   520.0  0.10750  0.12700   \n",
            "12    88466802         B  10.650  25.22   68.01   347.0  0.09657  0.07234   \n",
            "13      911150         B  14.530  19.34   94.25   659.7  0.08388  0.07800   \n",
            "14     9110944         B  14.800  17.66   95.88   674.8  0.09179  0.08890   \n",
            "15     9113156         B  14.400  26.99   92.25   646.1  0.06995  0.05223   \n",
            "16      859711         B   8.888  14.64   58.79   244.0  0.09783  0.15310   \n",
            "17     9013579         B  13.460  28.21   85.89   562.1  0.07517  0.04726   \n",
            "18    86973701         B  14.950  18.77   97.84   689.5  0.08138  0.11670   \n",
            "19    85638502         M  13.170  21.81   85.42   531.5  0.09714  0.10470   \n",
            "\n",
            "         f7       f8  ...     f21    f22     f23     f24     f25      f26  \\\n",
            "0   0.04568  0.02925  ...   8.952  22.44   56.65   240.1  0.1347  0.07767   \n",
            "1   0.02685  0.03515  ...  13.360  23.39   85.10   553.6  0.1137  0.07974   \n",
            "2   0.02669  0.01393  ...  13.300  24.99   85.22   546.3  0.1280  0.18800   \n",
            "3   0.02047  0.01257  ...  10.850  31.24   68.73   359.4  0.1526  0.11930   \n",
            "4   0.13210  0.02168  ...   9.092  29.72   58.08   249.8  0.1630  0.43100   \n",
            "5   0.15580  0.09176  ...  19.200  41.85  128.50  1153.0  0.2226  0.52090   \n",
            "6   0.01652  0.01667  ...  10.510  19.16   65.74   335.9  0.1504  0.09515   \n",
            "7   0.36350  0.16890  ...  36.040  31.37  251.20  4254.0  0.1357  0.42560   \n",
            "8   0.03102  0.02957  ...  16.110  29.11  102.90   803.7  0.1115  0.17660   \n",
            "9   0.25650  0.15040  ...  25.300  31.86  171.10  1938.0  0.1592  0.44920   \n",
            "10  0.12180  0.05182  ...  16.820  28.12  119.40   888.7  0.1637  0.57750   \n",
            "11  0.04568  0.03110  ...  14.500  20.49   96.09   630.5  0.1312  0.27760   \n",
            "12  0.02379  0.01615  ...  12.250  35.19   77.98   455.7  0.1499  0.13980   \n",
            "13  0.08817  0.02925  ...  16.300  28.39  108.10   830.5  0.1089  0.26490   \n",
            "14  0.04069  0.02260  ...  16.430  22.74  105.90   829.5  0.1226  0.18810   \n",
            "15  0.03476  0.01737  ...  14.690  31.98  100.40   734.6  0.1017  0.14600   \n",
            "16  0.08606  0.02872  ...   9.733  15.67   62.56   284.4  0.1207  0.24360   \n",
            "17  0.01271  0.01117  ...  14.690  35.63   97.11   680.6  0.1108  0.14570   \n",
            "18  0.09050  0.03562  ...  16.250  25.47  107.10   809.7  0.0997  0.25210   \n",
            "19  0.08259  0.05252  ...  16.230  29.89  105.50   740.7  0.1503  0.39040   \n",
            "\n",
            "        f27      f28     f29      f30  \n",
            "0   0.18900  0.07283  0.3142  0.08116  \n",
            "1   0.06120  0.07160  0.1978  0.06915  \n",
            "2   0.14710  0.06913  0.2535  0.07993  \n",
            "3   0.06141  0.03770  0.2872  0.08304  \n",
            "4   0.53810  0.07879  0.3322  0.14860  \n",
            "5   0.46460  0.20130  0.4432  0.10860  \n",
            "6   0.07161  0.07222  0.2757  0.08178  \n",
            "7   0.68330  0.26250  0.2641  0.07427  \n",
            "8   0.09189  0.06946  0.2522  0.07246  \n",
            "9   0.53440  0.26850  0.5558  0.10240  \n",
            "10  0.69560  0.15460  0.4761  0.14020  \n",
            "11  0.18900  0.07283  0.3184  0.08183  \n",
            "12  0.11250  0.06136  0.3409  0.08147  \n",
            "13  0.37790  0.09594  0.2471  0.07463  \n",
            "14  0.20600  0.08308  0.3600  0.07285  \n",
            "15  0.14720  0.05563  0.2345  0.06464  \n",
            "16  0.14340  0.04786  0.2254  0.10840  \n",
            "17  0.07934  0.05781  0.2694  0.07061  \n",
            "18  0.25000  0.08405  0.2852  0.09218  \n",
            "19  0.37280  0.16070  0.3693  0.09618  \n",
            "\n",
            "[20 rows x 32 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVPfRoS1mlNS",
        "colab_type": "text"
      },
      "source": [
        "If the data has outliers median would be the best choice because considering mean would not provide a better measure of central tendency,and that is the reason why I chose median over mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoiRFEoTmlNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "minmaxscaler=preprocessing.MinMaxScaler()\n",
        "array_train=np.array(data_train.iloc[:, 2:])\n",
        "data_train.iloc[:,2:]=minmaxscaler.fit_transform(array_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "c7o9ttx6mlNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "array_test=data_test.iloc[:, 2:]\n",
        "data_test.iloc[:,2:]=minmaxscaler.fit_transform(array_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFYVeOBGmlNk",
        "colab_type": "code",
        "colab": {},
        "outputId": "b9761b16-6c1a-4cb0-ae86-a55577030fb8"
      },
      "source": [
        "data_test['Diagnosis']= data_test['Diagnosis'].replace('M', 0)\n",
        "data_test['Diagnosis']= data_test['Diagnosis'].replace('B', 1)\n",
        "\n",
        "data_test['Diagnosis']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1\n",
              "1     0\n",
              "2     1\n",
              "3     1\n",
              "4     1\n",
              "5     0\n",
              "6     1\n",
              "7     0\n",
              "8     1\n",
              "9     0\n",
              "10    0\n",
              "11    1\n",
              "12    1\n",
              "13    1\n",
              "14    1\n",
              "15    1\n",
              "16    1\n",
              "17    1\n",
              "18    1\n",
              "19    0\n",
              "Name: Diagnosis, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejHVq5nXmlN1",
        "colab_type": "code",
        "colab": {},
        "outputId": "c7d922a5-2a3a-4e49-f795-b9a336d91cae"
      },
      "source": [
        "data_train['Diagnosis']= data_train['Diagnosis'].replace('M', 0)\n",
        "data_train['Diagnosis']= data_train['Diagnosis'].replace('B', 1)\n",
        "\n",
        "data_train['Diagnosis']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     1\n",
              "1     0\n",
              "2     1\n",
              "3     1\n",
              "4     1\n",
              "5     1\n",
              "6     1\n",
              "7     1\n",
              "8     0\n",
              "9     1\n",
              "10    0\n",
              "11    1\n",
              "12    1\n",
              "13    1\n",
              "14    1\n",
              "15    1\n",
              "16    1\n",
              "17    1\n",
              "18    1\n",
              "19    0\n",
              "20    1\n",
              "21    1\n",
              "22    0\n",
              "23    0\n",
              "24    1\n",
              "25    1\n",
              "26    1\n",
              "27    0\n",
              "28    1\n",
              "29    1\n",
              "     ..\n",
              "70    1\n",
              "71    1\n",
              "72    0\n",
              "73    1\n",
              "74    0\n",
              "75    0\n",
              "76    1\n",
              "77    1\n",
              "78    1\n",
              "79    0\n",
              "80    0\n",
              "81    1\n",
              "82    1\n",
              "83    0\n",
              "84    1\n",
              "85    0\n",
              "86    1\n",
              "87    0\n",
              "88    0\n",
              "89    0\n",
              "90    0\n",
              "91    0\n",
              "92    1\n",
              "93    0\n",
              "94    0\n",
              "95    0\n",
              "96    0\n",
              "97    1\n",
              "98    1\n",
              "99    1\n",
              "Name: Diagnosis, Length: 100, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "H064kJGvmlOG",
        "colab_type": "code",
        "colab": {},
        "outputId": "eca36a95-56da-4f0b-feac-d6649c089694"
      },
      "source": [
        "predictors = ['f1', 'f2','f3','f4', 'f5', 'f6', 'f7','f8','f9', 'f10', 'f11', 'f12','f13','f14', 'f15', 'f16', 'f17','f18','f19', 'f20', 'f21', 'f22','f23','f24', 'f25', 'f26', 'f27','f28','f29', 'f30']\n",
        "response = ['Diagnosis']\n",
        "print(data_train[predictors])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          f1        f2        f3        f4        f5        f6        f7  \\\n",
            "0   0.359671  0.223557  0.336869  0.251707  0.169829  0.112411  0.042753   \n",
            "1   0.718141  0.162587  0.705311  0.658074  0.493492  0.379775  0.460866   \n",
            "2   0.295066  0.225404  0.292260  0.198799  0.146621  0.251285  0.256684   \n",
            "3   0.144703  0.064665  0.143863  0.084040  0.488161  0.180511  0.038882   \n",
            "4   0.310503  0.448961  0.292098  0.211806  0.070880  0.101785  0.035978   \n",
            "5   0.091704  0.513626  0.089297  0.050141  0.369610  0.113245  0.087815   \n",
            "6   0.237894  0.302079  0.228465  0.151130  0.841618  0.168982  0.133443   \n",
            "7   0.304214  0.139492  0.281169  0.207039  0.230359  0.048305  0.004251   \n",
            "8   0.612944  0.512240  0.584683  0.527425  0.399404  0.255765  0.265654   \n",
            "9   0.240752  0.342725  0.224498  0.152307  0.216089  0.083588  0.040039   \n",
            "10  0.562632  0.195381  0.542584  0.456862  0.464795  0.320392  0.226588   \n",
            "11  0.337945  0.254965  0.317600  0.231285  0.125764  0.129011  0.037324   \n",
            "12  0.349951  0.295150  0.327558  0.241231  0.298259  0.131685  0.027009   \n",
            "13  0.241324  0.191224  0.236804  0.145892  0.736553  0.337408  0.021131   \n",
            "14  0.099251  0.470670  0.098203  0.053319  0.557786  0.188533  0.044759   \n",
            "15  0.335087  0.324249  0.309747  0.231580  0.167006  0.059314  0.040629   \n",
            "16  0.191584  0.179677  0.179971  0.114113  0.498197  0.118037  0.049268   \n",
            "17  0.148133  0.255427  0.140301  0.085982  0.400031  0.092027  0.017378   \n",
            "18  0.300783  0.157506  0.292260  0.197446  0.404422  0.264655  0.081725   \n",
            "19  0.674690  1.000000  0.647830  0.614524  0.241022  0.311015  0.263765   \n",
            "20  0.392259  0.547806  0.391920  0.281485  0.249334  0.380470  0.236384   \n",
            "21  0.431136  0.096536  0.406331  0.321740  0.203701  0.134949  0.132569   \n",
            "22  0.357384  0.406467  0.349255  0.247587  0.582876  0.312057  0.259280   \n",
            "23  0.455720  0.438337  0.430861  0.345927  0.425592  0.224163  0.149116   \n",
            "24  0.391687  0.131178  0.374919  0.282486  0.462443  0.228296  0.187569   \n",
            "25  0.238465  0.176905  0.243928  0.149482  0.727144  0.368662  0.081064   \n",
            "26  0.219599  0.379677  0.212273  0.135887  0.260781  0.186762  0.122962   \n",
            "27  0.382540  0.413395  0.376457  0.272893  0.449898  0.314835  0.215092   \n",
            "28  0.114859  0.053118  0.107351  0.064560  0.371178  0.061050  0.163315   \n",
            "29  0.082557  0.140416  0.090835  0.048317  0.131410  0.222496  0.134836   \n",
            "..       ...       ...       ...       ...       ...       ...       ...   \n",
            "70  0.336802  0.573210  0.317438  0.232109  0.370550  0.152903  0.063691   \n",
            "71  0.049683  0.469284  0.054080  0.025306  0.869845  0.229858  0.064304   \n",
            "72  0.336230  0.654042  0.324401  0.237700  0.408499  0.214162  0.197081   \n",
            "73  0.057001  0.231409  0.063633  0.030661  0.724008  0.203396  0.091025   \n",
            "74  0.269910  0.225404  0.280036  0.175553  0.924729  0.508960  0.365974   \n",
            "75  0.487165  0.401386  0.500486  0.374882  0.529559  0.575983  0.521294   \n",
            "76  0.304214  0.358891  0.300437  0.196092  0.617375  0.371440  0.220167   \n",
            "77  0.000000  0.677598  0.000000  0.000000  0.190528  0.087998  0.163315   \n",
            "78  0.272197  0.278522  0.252672  0.178202  0.255292  0.051743  0.004048   \n",
            "79  0.372820  0.503464  0.369576  0.267302  0.461032  0.299903  0.304838   \n",
            "80  0.301355  0.508083  0.319948  0.200683  0.916889  0.589526  0.432304   \n",
            "81  0.113830  0.341801  0.103222  0.064324  0.010507  0.000000  0.163315   \n",
            "82  0.259619  0.884527  0.238342  0.168668  0.127960  0.036602  0.163315   \n",
            "83  0.831342  0.408776  0.848608  0.782839  1.000000  0.879844  1.000000   \n",
            "84  0.111715  0.393533  0.105813  0.062912  0.252156  0.093277  0.048654   \n",
            "85  0.648391  0.646189  0.650259  0.544492  0.344676  0.679122  0.490844   \n",
            "86  0.457435  0.021247  0.443815  0.334510  0.556218  0.369704  0.274623   \n",
            "87  0.640958  0.484527  0.612209  0.559793  0.333386  0.275941  0.248422   \n",
            "88  0.482019  0.495150  0.471341  0.371351  0.501333  0.364495  0.239688   \n",
            "89  0.545481  0.674365  0.521535  0.441384  0.227066  0.303723  0.277692   \n",
            "90  0.328798  0.461894  0.327234  0.223870  0.513878  0.354424  0.244409   \n",
            "91  0.441999  0.400000  0.434909  0.335511  0.633056  0.343312  0.339537   \n",
            "92  0.096850  0.787991  0.091321  0.054437  0.194449  0.091228  0.163315   \n",
            "93  0.738151  0.301617  0.702882  0.680438  0.401913  0.292263  0.353936   \n",
            "94  0.659825  0.722864  0.647021  0.578625  0.394857  0.515558  0.384622   \n",
            "95  1.000000  0.650808  1.000000  1.000000  0.587580  0.844076  0.781655   \n",
            "96  0.763879  0.727483  0.766839  0.666314  0.709895  0.902070  0.580542   \n",
            "97  0.235607  0.306697  0.223122  0.149423  0.233495  0.114530  0.056940   \n",
            "98  0.198445  0.856813  0.183695  0.121940  0.088756  0.042159  0.163315   \n",
            "99  0.382540  0.264203  0.373786  0.272128  0.449741  0.314141  0.182494   \n",
            "\n",
            "          f8        f9       f10  ...       f21       f22       f23       f24  \\\n",
            "0   0.128987  0.353610  0.092522  ...  0.265946  0.166401  0.254708  0.163539   \n",
            "1   0.557822  0.500668  0.186629  ...  0.613824  0.082082  0.614961  0.492211   \n",
            "2   0.169676  0.248663  0.203105  ...  0.219897  0.178537  0.226170  0.128932   \n",
            "3   0.091275  0.494652  0.403992  ...  0.105002  0.049505  0.103928  0.054340   \n",
            "4   0.052019  0.436497  0.188213  ...  0.247253  0.458639  0.246683  0.148479   \n",
            "5   0.053453  0.422460  0.406210  ...  0.035016  0.349729  0.036821  0.017730   \n",
            "6   0.390986  0.638369  0.184094  ...  0.151507  0.156180  0.144244  0.083234   \n",
            "7   0.031449  0.262701  0.070976  ...  0.257739  0.105398  0.238077  0.157418   \n",
            "8   0.360276  0.422460  0.137199  ...  0.612912  0.552220  0.572251  0.497774   \n",
            "9   0.057147  0.540107  0.258555  ...  0.183422  0.230917  0.171488  0.103561   \n",
            "10  0.421254  0.453877  0.270913  ...  0.498473  0.173108  0.503009  0.363131   \n",
            "11  0.077767  0.195856  0.208809  ...  0.255004  0.290323  0.251796  0.151595   \n",
            "12  0.045679  0.370321  0.188847  ...  0.292390  0.302779  0.279104  0.177263   \n",
            "13  0.081296  0.935160  0.407795  ...  0.170200  0.115618  0.167734  0.092062   \n",
            "14  0.065693  0.439171  0.508555  ...  0.060776  0.557649  0.063806  0.030675   \n",
            "15  0.051027  0.342914  0.071610  ...  0.292846  0.383264  0.270109  0.182159   \n",
            "16  0.125183  0.338235  0.435678  ...  0.103634  0.086873  0.096033  0.055082   \n",
            "17  0.013067  0.551471  0.287389  ...  0.099074  0.244970  0.090274  0.050779   \n",
            "18  0.098442  0.395722  0.615970  ...  0.211690  0.089748  0.195496  0.121662   \n",
            "19  0.348697  0.245989  0.006020  ...  0.858205  1.000000  0.796156  0.826780   \n",
            "20  0.188753  0.263369  0.270279  ...  0.291935  0.420632  0.315343  0.180082   \n",
            "21  0.156830  0.199866  0.073511  ...  0.378562  0.058128  0.365819  0.253969   \n",
            "22  0.339104  0.407086  0.396071  ...  0.363060  0.533376  0.367113  0.230453   \n",
            "23  0.268863  0.372995  0.080798  ...  0.503032  0.597892  0.466770  0.378709   \n",
            "24  0.222550  0.453877  0.109632  ...  0.350294  0.100287  0.332169  0.227745   \n",
            "25  0.148339  0.625000  0.806401  ...  0.185702  0.076972  0.188248  0.097033   \n",
            "26  0.061502  0.649064  0.308302  ...  0.182054  0.411370  0.189348  0.101076   \n",
            "27  0.304700  0.547460  0.347275  ...  0.330689  0.535292  0.338640  0.214540   \n",
            "28  0.183019  0.563503  0.357414  ...  0.070351  0.000000  0.060571  0.035200   \n",
            "29  0.102963  0.856283  0.528834  ...  0.042539  0.163845  0.054488  0.023034   \n",
            "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
            "70  0.117464  0.403743  0.160646  ...  0.284412  0.478761  0.261503  0.178709   \n",
            "71  0.033819  0.513369  0.463561  ...  0.022250  0.413286  0.031644  0.009607   \n",
            "72  0.230269  0.366979  0.182826  ...  0.360781  0.676142  0.332816  0.244251   \n",
            "73  0.088849  0.393717  0.741128  ...  0.025122  0.176302  0.047305  0.013501   \n",
            "74  0.428753  0.686497  0.734791  ...  0.284412  0.308208  0.299165  0.183086   \n",
            "75  0.489676  0.729278  0.406844  ...  0.379018  0.285851  0.420177  0.256565   \n",
            "76  0.291688  0.651738  0.382446  ...  0.233119  0.343341  0.239759  0.127300   \n",
            "77  0.183019  0.541444  0.630862  ...  0.000000  0.537209  0.000000  0.000000   \n",
            "78  0.018183  0.103610  0.120089  ...  0.194365  0.179176  0.176729  0.109866   \n",
            "79  0.291413  0.551471  0.263308  ...  0.310628  0.519323  0.381997  0.204599   \n",
            "80  0.498442  0.862299  0.663815  ...  0.292390  0.531140  0.317285  0.182233   \n",
            "81  0.183019  0.396390  0.365336  ...  0.040624  0.214947  0.032874  0.020697   \n",
            "82  0.183019  0.428476  0.211027  ...  0.199380  0.764931  0.180612  0.115319   \n",
            "83  0.987870  1.000000  0.552915  ...  0.881001  0.444267  0.968291  0.783383   \n",
            "84  0.035782  0.347594  0.298162  ...  0.084485  0.366975  0.077202  0.043101   \n",
            "85  0.531964  0.835561  0.332383  ...  0.684494  0.609071  0.778037  0.520401   \n",
            "86  0.325210  0.483957  0.305767  ...  0.361692  0.003194  0.358053  0.224889   \n",
            "87  0.421144  0.348930  0.052915  ...  0.719601  0.398595  0.703617  0.600148   \n",
            "88  0.292240  0.735294  0.189163  ...  0.473852  0.561482  0.469359  0.343843   \n",
            "89  0.299021  0.272059  0.035805  ...  0.515342  0.682210  0.489420  0.384273   \n",
            "90  0.282646  0.441176  0.356464  ...  0.294214  0.380709  0.324403  0.182641   \n",
            "91  0.428642  0.582219  0.159062  ...  0.464278  0.380070  0.438297  0.336795   \n",
            "92  0.183019  0.455882  0.242395  ...  0.064423  0.643245  0.060377  0.030638   \n",
            "93  0.475727  0.341578  0.058302  ...  0.742853  0.289684  0.709442  0.629822   \n",
            "94  0.401406  0.530080  0.306401  ...  0.687229  0.536570  0.674497  0.580490   \n",
            "95  1.000000  0.514037  0.471483  ...  0.953951  0.623443  1.000000  0.858309   \n",
            "96  0.807581  0.892380  0.666667  ...  0.802581  0.618972  0.772213  0.682864   \n",
            "97  0.108477  0.544786  0.133397  ...  0.181599  0.372086  0.175888  0.100074   \n",
            "98  0.183019  0.000000  0.065906  ...  0.129622  0.772916  0.116612  0.071068   \n",
            "99  0.215603  0.569519  0.354246  ...  0.346191  0.236666  0.350935  0.227782   \n",
            "\n",
            "         f25       f26       f27       f28       f29       f30  \n",
            "0   0.276344  0.101777  0.051541  0.227005  0.178852  0.116078  \n",
            "1   0.567864  0.243069  0.407820  0.524374  0.250392  0.255660  \n",
            "2   0.240847  0.279589  0.338550  0.299330  0.135864  0.176208  \n",
            "3   0.541284  0.188357  0.076409  0.170744  0.430185  0.262004  \n",
            "4   0.348367  0.187810  0.066726  0.078173  0.351741  0.219755  \n",
            "5   0.335505  0.076172  0.069841  0.017804  0.272670  0.214564  \n",
            "6   0.555003  0.077129  0.060971  0.254025  0.205209  0.018457  \n",
            "7   0.260911  0.042647  0.004699  0.053633  0.169752  0.097332  \n",
            "8   0.645889  0.273844  0.404125  0.433320  0.484154  0.245854  \n",
            "9   0.370659  0.084488  0.082618  0.155939  0.382491  0.216727  \n",
            "10  0.465832  0.347294  0.248266  0.461820  0.373392  0.290411  \n",
            "11  0.323502  0.179057  0.096314  0.240700  0.244744  0.305696  \n",
            "12  0.381806  0.147324  0.071129  0.111818  0.250392  0.184138  \n",
            "13  0.498414  0.220500  0.018405  0.099308  0.488861  0.242970  \n",
            "14  0.717054  0.190272  0.084825  0.164119  0.411672  0.476712  \n",
            "15  0.290063  0.066926  0.097370  0.114817  0.241293  0.051622  \n",
            "16  0.432393  0.075379  0.027022  0.082282  0.105428  0.203028  \n",
            "17  0.491554  0.083613  0.031731  0.011030  0.349859  0.215429  \n",
            "18  0.368945  0.232810  0.138552  0.139431  0.271729  0.603461  \n",
            "19  0.597016  0.523054  0.407292  0.524374  0.359900  0.239798  \n",
            "20  0.269485  0.396397  0.372129  0.331902  0.217132  0.302668  \n",
            "21  0.437538  0.261671  0.298212  0.305993  0.236586  0.226244  \n",
            "22  0.772786  0.450698  0.415106  0.599141  0.506119  0.669791  \n",
            "23  0.602161  0.447826  0.291877  0.493282  0.587072  0.393079  \n",
            "24  0.438395  0.226382  0.248582  0.262909  0.332601  0.164816  \n",
            "25  0.610735  0.306944  0.084909  0.146537  0.364292  0.642394  \n",
            "26  0.460688  0.227613  0.255763  0.169560  0.460935  0.343043  \n",
            "27  0.616737  0.376564  0.322710  0.502165  0.361468  0.497188  \n",
            "28  0.447826  0.061208  0.286808  0.352630  0.482899  0.323864  \n",
            "29  0.233130  0.192187  0.132955  0.111189  0.538437  0.372747  \n",
            "..       ...       ...       ...       ...       ...       ...  \n",
            "70  0.432393  0.170166  0.096208  0.188437  0.337622  0.151118  \n",
            "71  0.795078  0.194923  0.080485  0.025725  0.441167  0.260562  \n",
            "72  0.474406  0.228433  0.320071  0.361143  0.340446  0.215141  \n",
            "73  0.867959  0.286017  0.244781  0.311545  0.426734  0.614996  \n",
            "74  0.925405  0.680623  0.550902  0.567309  0.759021  0.942322  \n",
            "75  0.570436  0.572978  0.608135  0.579894  0.549733  0.465898  \n",
            "76  0.541284  0.326230  0.257136  0.388533  0.485409  0.347801  \n",
            "77  0.466690  0.076746  0.286808  0.352630  0.468152  0.581543  \n",
            "78  0.336363  0.030419  0.005903  0.025947  0.110135  0.038789  \n",
            "79  0.629598  0.542340  0.533057  0.458489  0.635394  0.610671  \n",
            "80  0.849953  0.701413  0.554598  0.685383  0.882334  0.694304  \n",
            "81  0.000000  0.000000  0.286808  0.352630  0.107625  0.094304  \n",
            "82  0.197634  0.033976  0.286808  0.352630  0.264512  0.120836  \n",
            "83  0.848238  0.919711  1.000000  1.000000  0.780985  0.560058  \n",
            "84  0.417817  0.093242  0.084170  0.065255  0.309382  0.290555  \n",
            "85  0.458973  0.980851  0.750161  0.845653  0.973957  0.645278  \n",
            "86  0.711052  0.370136  0.408243  0.460340  0.310951  0.328190  \n",
            "87  0.412673  0.282187  0.269174  0.585076  0.309068  0.098630  \n",
            "88  0.585870  0.380531  0.301380  0.408521  0.600565  0.247152  \n",
            "89  0.621024  0.526473  0.517217  0.566569  0.293066  0.294016  \n",
            "90  0.770213  0.540562  0.516584  0.758670  0.389394  0.692862  \n",
            "91  0.715339  0.290121  0.385751  0.483288  0.398808  0.304831  \n",
            "92  0.309783  0.060579  0.286808  0.352630  0.285221  0.153425  \n",
            "93  0.729058  0.380531  0.424610  0.704630  0.352369  0.165826  \n",
            "94  0.683615  0.864042  0.628621  0.583596  0.660810  0.767844  \n",
            "95  0.738489  0.793738  0.669275  0.984084  0.247568  0.664023  \n",
            "96  0.668181  1.000000  0.701376  0.997409  0.794478  1.000000  \n",
            "97  0.563577  0.203129  0.124402  0.261206  0.481644  0.158904  \n",
            "98  0.184344  0.037819  0.286808  0.352630  0.000000  0.000000  \n",
            "99  0.499271  0.423753  0.381950  0.446275  0.466583  0.412257  \n",
            "\n",
            "[100 rows x 30 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4r9QjpJmlOe",
        "colab_type": "text"
      },
      "source": [
        "#  Logistic Regression "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n27Al9CgmlO4",
        "colab_type": "code",
        "colab": {},
        "outputId": "dea9f576-9b4c-444a-d5a7-c1ba374cd7aa"
      },
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lambda_val = 0.1\n",
        "#Initialize the Logitic regression model with l2 penalty\n",
        "lr = LogisticRegression(C=1/lambda_val, penalty='l2')\n",
        "lr.fit(data_train[predictors], data_train['Diagnosis'])\n",
        "y_predict = lr.predict(data_test[predictors])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, data_test['Diagnosis'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.95\n",
            "Model Coeff: [ 7.25821413 -0.57071254 -1.59675227 -0.7073427  -1.13357305 -0.24676961\n",
            " -0.57854985 -1.91187289 -2.4210991  -0.01399259  1.89987611 -0.82495074\n",
            "  1.06817933 -1.01804489 -1.51635977  1.05407205  0.75406307  0.84382813\n",
            "  0.76007611  1.62095715  0.56682818 -1.62866712 -3.19431076 -1.75867884\n",
            " -1.81576167 -0.94735976 -1.39030257 -1.53231015 -1.91484531 -1.4580444\n",
            " -1.34982096]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOwX6kKfmlPJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "abd50fb0-e1d8-46f0-968a-eba2d34d4ec8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "alpha_val = 0.1\n",
        "#Initialize the Logitic regression model with l2 penalty\n",
        "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
        "lr.fit(data_train[predictors], data_train['Diagnosis'])\n",
        "y_predict = lr.predict(data_test[predictors])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, data_test['Diagnosis'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.9\n",
            "Model Coeff: [ 16.20876256   0.           0.           0.           0.\n",
            "   0.           0.           0.          -2.79733444   0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.          -6.99014716  -9.76388191 -17.20534378   0.\n",
            "  -4.81428218   0.           0.          -0.15492962   0.\n",
            "  -3.32732534]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aszzx0MZmlPf",
        "colab_type": "code",
        "colab": {},
        "outputId": "36b11dc3-8f89-4764-c73b-129c11bff0a0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = data_test['Diagnosis']\n",
        "print(classification_report(y_true, y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         6\n",
            "           1       0.88      1.00      0.93        14\n",
            "\n",
            "   micro avg       0.90      0.90      0.90        20\n",
            "   macro avg       0.94      0.83      0.87        20\n",
            "weighted avg       0.91      0.90      0.89        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU8lOWkQmlQU",
        "colab_type": "code",
        "colab": {},
        "outputId": "b7b37ad5-db58-48f1-b924-2e11a41c9af7"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(data_test['Diagnosis'],y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  2],\n",
              "       [ 0, 14]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bafdF3P8mlQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "def runLRmodel(trials, data, predictors, label, penalty_type, penalty_score):\n",
        "\n",
        "   model_acc     = 0\n",
        "   \n",
        "\n",
        "   for i in range(0,trials):\n",
        "      Dtrain, Dtest = train_test_split(data, test_size=0.3)\n",
        "      lr = LogisticRegression(C=1/penalty_score, penalty=penalty_type)\n",
        "      lr.fit(Dtrain[predictors], Dtrain[label])\n",
        "      y_predict = lr.predict(Dtest[predictors])\n",
        "      model_acc += f1_score(y_predict, Dtest[label])\n",
        "      \n",
        "\n",
        "   model_acc /= trials\n",
        "   \n",
        "\n",
        "   return np.round(model_acc, decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmtJAAInmlQq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJf3QWpHmlQr",
        "colab_type": "code",
        "colab": {},
        "outputId": "0b41cc90-a8ad-481d-a0fb-0a11d26b32d3"
      },
      "source": [
        "\n",
        "\n",
        "alpha_vals = [0.1,1,3,10,33,100,333,1000, 3333, 10000, 33333]\n",
        "l1_acc = np.zeros(len(alpha_vals))\n",
        "index = 0\n",
        "#L2 regularization\n",
        "for l in alpha_vals:\n",
        "   l1_acc[index] = runLRmodel(10,data_train, predictors, 'Diagnosis', 'l1', np.float(l))\n",
        "   index += 1\n",
        "\n",
        "print(\"Acc: {}\".format(l1_acc))\n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_l1  = np.argmax(l1_acc)\n",
        "best_alpha = alpha_vals[max_index_l1]\n",
        "print(\"Best Alpha: {}\".format(best_alpha))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: [0.99 0.96 0.9  0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "Best Alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnyj83uHmlQ0",
        "colab_type": "text"
      },
      "source": [
        "# 1.3B Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "meDpBsWTmlQ0",
        "colab_type": "code",
        "colab": {},
        "outputId": "75f18793-6a66-4c73-bb91-6605d02ea33c"
      },
      "source": [
        "\n",
        "\n",
        "lambda_vals = [0.001, 0.003, 0.01, 0.03, 0.1,0.3,1,3,10,33]\n",
        "l2_acc = np.zeros(len(lambda_vals))\n",
        "index = 0\n",
        "#L2 regularization\n",
        "for l in lambda_vals:\n",
        "   l2_acc[index] = runLRmodel(10,data_train, predictors, 'Diagnosis', 'l2', np.float(l))\n",
        "   index += 1\n",
        "\n",
        "print(\"Acc: {}\".format(l2_acc))\n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_l2  = np.argmax(l2_acc)\n",
        "best_lambda = lambda_vals[max_index_l2]\n",
        "print(\"Best Lambda: {}\".format(best_lambda))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: [0.98 0.99 0.99 0.98 0.98 0.96 0.95 0.95 0.92 0.88]\n",
            "Best Lambda: 0.003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc6C3T6zmlQ4",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79UTPWwumlQ5",
        "colab_type": "code",
        "colab": {},
        "outputId": "c0cd317c-7161-44af-ed32-54f3d96a4d26"
      },
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "lambda_val = 0.003\n",
        "#Initialize the Logitic regression model with l2 penalty\n",
        "lr = LogisticRegression(C=1/lambda_val, penalty='l2')\n",
        "lr.fit(data_train[predictors], data_train['Diagnosis'])\n",
        "y_predict = lr.predict(data_test[predictors])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, data_test['Diagnosis'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.95\n",
            "Model Coeff: [ 19.94503261  -1.89938266  -3.56778988  -2.32063607  -3.7957463\n",
            "   1.57110675  -0.02840747  -5.68851017  -6.96448269  -0.78450117\n",
            "   7.4900836   -1.67388226   2.77520743  -2.93111023  -3.75571881\n",
            "  -1.95451347   2.08803674   2.61324884   2.30208323   5.370148\n",
            "  -1.33729949  -4.47516756 -10.41121119  -4.97823307  -5.30924984\n",
            "  -4.3073315   -2.10355215  -1.18474514  -2.38005483  -4.36795951\n",
            "  -5.89786837]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IP_0sOPOmlQ8",
        "colab_type": "code",
        "colab": {},
        "outputId": "7504300a-d589-4a0f-d24f-d6b071bec99e"
      },
      "source": [
        "features1=pd.DataFrame(lr.coef_.transpose(),columns= [\"\"])\n",
        "sort=features1.sort_values(by=\"\", ascending=False)\n",
        "print(\"The top 5 features lamda:\", sort.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The top 5 features lamda:             \n",
            "9   7.490084\n",
            "18  5.370148\n",
            "11  2.775207\n",
            "16  2.613249\n",
            "17  2.302083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLG7XeYwmlRD",
        "colab_type": "code",
        "colab": {},
        "outputId": "9d9c0715-132a-46bd-ff13-341fde8e569a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = data_test['Diagnosis']\n",
        "print(classification_report(y_true, y_predict))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.83      0.91         6\n",
            "           1       0.93      1.00      0.97        14\n",
            "\n",
            "   micro avg       0.95      0.95      0.95        20\n",
            "   macro avg       0.97      0.92      0.94        20\n",
            "weighted avg       0.95      0.95      0.95        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaOZy6QLmlRJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "f4ae75f1-420c-485e-9819-34b5fe06e982"
      },
      "source": [
        "alpha_val = 0.1\n",
        "#Initialize the Logitic regression model with l2 penalty\n",
        "lr = LogisticRegression(C=1/alpha_val, penalty='l1')\n",
        "lr.fit(data_train[predictors], data_train['Diagnosis'])\n",
        "y_predict = lr.predict(data_test[predictors])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, data_test['Diagnosis'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.9\n",
            "Model Coeff: [ 16.22443839   0.           0.           0.           0.\n",
            "   0.           0.           0.          -2.82538694   0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.          -7.5897989   -9.76581918 -16.58124943   0.\n",
            "  -4.81282941   0.           0.          -0.18348465   0.\n",
            "  -3.34246062]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qISiiB6AmlRL",
        "colab_type": "code",
        "colab": {},
        "outputId": "72ae10e6-b05e-476f-bcac-d8ce42ed1b35"
      },
      "source": [
        "#The top 5 features selected in decreasing order of feature weights\n",
        "features=pd.DataFrame(lr.coef_.transpose(),columns= [\"\"])\n",
        "sort=features.sort_values(by=\"\", ascending=False)\n",
        "print(\"The top 5 features alpha:\", sort.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The top 5 features alpha:        \n",
            "0   0.0\n",
            "13  0.0\n",
            "28  0.0\n",
            "26  0.0\n",
            "25  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrbPB9vGmlRO",
        "colab_type": "code",
        "colab": {},
        "outputId": "4a85f4bd-fe9d-41c5-aebe-46f6f7c4934f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = data_test['Diagnosis']\n",
        "print(classification_report(y_true, y_predict))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         6\n",
            "           1       0.88      1.00      0.93        14\n",
            "\n",
            "   micro avg       0.90      0.90      0.90        20\n",
            "   macro avg       0.94      0.83      0.87        20\n",
            "weighted avg       0.91      0.90      0.89        20\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOw-83BumlRR",
        "colab_type": "code",
        "colab": {},
        "outputId": "f6bde839-3105-41ee-d2d6-d152add33b86"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(data_test['Diagnosis'],y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  2],\n",
              "       [ 0, 14]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9CmtrmsmlRa",
        "colab_type": "text"
      },
      "source": [
        "# Multi Class Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wcnJMaxmlRc",
        "colab_type": "code",
        "colab": {},
        "outputId": "d16a79fe-e556-4d92-c074-d8881ecc6784"
      },
      "source": [
        "\n",
        "data_mnist= pd.read_csv(\"data/mnist/reduced_mnist.csv\")\n",
        "data_mnist.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
              "0      1       0       0       0       0       0       0       0       0   \n",
              "1      9       0       0       0       0       0       0       0       0   \n",
              "2      1       0       0       0       0       0       0       0       0   \n",
              "3      4       0       0       0       0       0       0       0       0   \n",
              "4      2       0       0       0       0       0       0       0       0   \n",
              "\n",
              "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
              "0       0  ...         0         0         0         0         0         0   \n",
              "1       0  ...         0         0         0         0         0         0   \n",
              "2       0  ...         0         0         0         0         0         0   \n",
              "3       0  ...         0         0         0         0         0         0   \n",
              "4       0  ...         0         0         0         0         0         0   \n",
              "\n",
              "   pixel780  pixel781  pixel782  pixel783  \n",
              "0         0         0         0         0  \n",
              "1         0         0         0         0  \n",
              "2         0         0         0         0  \n",
              "3         0         0         0         0  \n",
              "4         0         0         0         0  \n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN5hoai0mlRg",
        "colab_type": "code",
        "colab": {},
        "outputId": "512ae867-b650-4f31-e6a0-abcf72c76b56"
      },
      "source": [
        "data_points= len(data_mnist)\n",
        "print(\" Number of data_points:\", data_points)\n",
        "number_f= len(data_mnist.iloc[:, 1:].columns)\n",
        "print(\"Number of feature:\", number_f )\n",
        "unique_l= np.unique(data_mnist['label'].values)\n",
        "print(\"Unique Labels:\", unique_l )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of data_points: 2520\n",
            "Number of feature: 784\n",
            "Unique Labels: [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1yToVdemlRl",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svgYXX2JmlRm",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e44cacd-6b90-42c3-8000-f2567b3860eb"
      },
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "Mtrain,Mtest = train_test_split(data_mnist, test_size=0.3)\n",
        "\n",
        "print(Mtrain.shape)\n",
        "print(Mtest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1764, 785)\n",
            "(756, 785)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8x_0X4ZmlRp",
        "colab_type": "code",
        "colab": {},
        "outputId": "61bad7cd-6177-4c9a-946d-1efb12a74240"
      },
      "source": [
        "alpha_val = 1\n",
        "multi_lr = LogisticRegression(C=1/alpha_val, penalty='l1', multi_class='ovr')\n",
        "multi_lr.fit(Mtrain.iloc[:,1:],Mtrain['label'])\n",
        "y_predict =multi_lr .predict(Mtest.iloc[:,1:])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, Mtest['label'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.8386243386243386\n",
            "Model Coeff: [ 16.22443839   0.           0.           0.           0.\n",
            "   0.           0.           0.          -2.82538694   0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.          -7.5897989   -9.76581918 -16.58124943   0.\n",
            "  -4.81282941   0.           0.          -0.18348465   0.\n",
            "  -3.34246062]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwiEWQrtmlRt",
        "colab_type": "code",
        "colab": {},
        "outputId": "03714b05-acdb-4878-8788-d4baa6790c1e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = Mtest['label']\n",
        "print(classification_report(y_true, y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.88        68\n",
            "           1       0.88      0.97      0.92        72\n",
            "           2       0.88      0.78      0.83        86\n",
            "           3       0.87      0.86      0.86        78\n",
            "           4       0.78      0.82      0.80        72\n",
            "           5       0.89      0.76      0.82        75\n",
            "           6       0.89      0.88      0.89        66\n",
            "           7       0.91      0.87      0.89        83\n",
            "           8       0.68      0.73      0.70        85\n",
            "           9       0.82      0.82      0.82        71\n",
            "\n",
            "   micro avg       0.84      0.84      0.84       756\n",
            "   macro avg       0.84      0.84      0.84       756\n",
            "weighted avg       0.84      0.84      0.84       756\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4kKapAnmlRw",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07M_AYoLmlRw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtxzj0FdmlRx",
        "colab_type": "code",
        "colab": {},
        "outputId": "755f8331-9e62-47f9-afce-08fc04a0e7ac"
      },
      "source": [
        "predictors= data_mnist.iloc[:, 1:].columns\n",
        "predictors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
              "       'pixel7', 'pixel8', 'pixel9',\n",
              "       ...\n",
              "       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n",
              "       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n",
              "      dtype='object', length=784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6ltAvLXmlR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def runLRmodel(trials, data, predictors, label, penalty_type, penalty_score):\n",
        "\n",
        "   model_acc     = 0\n",
        "   model_acc_train=0\n",
        "\n",
        "   for i in range(0,trials):\n",
        "      Dtrain, Dtest = train_test_split(data, test_size=0.3)\n",
        "      lr = LogisticRegression(C=1/penalty_score,multi_class='ovr', penalty=penalty_type)\n",
        "      lr.fit(Dtrain[predictors], Dtrain[label])\n",
        "      y_predict = lr.predict(Dtest[predictors])\n",
        "      model_acc += f1_score(y_predict, Dtest[label],average='macro')\n",
        "      y_predtrain=lr.predict(Dtrain[predictors])\n",
        "      model_acc_train+=f1_score(y_predtrain,Dtrain[label],average='macro')\n",
        "      \n",
        "\n",
        "   model_acc /= trials\n",
        "   model_acc_train/=trials\n",
        "   \n",
        "\n",
        "   return np.round(model_acc, decimals=2),np.round(model_acc_train,decimals=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f2dVru9mlR5",
        "colab_type": "code",
        "colab": {},
        "outputId": "ef26e226-50a5-4ec9-fb9f-d08fe5d2a232"
      },
      "source": [
        "alpha_vals = [0.1, 1, 3, 10, 33, 100, 333, 1000, 3333, 10000, 33333]\n",
        "l1_acc = np.zeros(len(alpha_vals))\n",
        "l1_acc2=np.zeros(len(alpha_vals))\n",
        "index = 0\n",
        "#L2 regularization\n",
        "for l in alpha_vals:\n",
        "   l1_acc[index],l1_acc2[index] = runLRmodel(10,Mtrain, predictors, 'label', 'l1', np.float(l))\n",
        "   index += 1\n",
        "\n",
        "print(\"Validation Acc: {}\".format(l1_acc))\n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_l1  = np.argmax(l1_acc)\n",
        "best_alpha = alpha_vals[max_index_l1]\n",
        "print(\"Best Validation Alpha: {}\".format(best_alpha))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation Acc: [0.83 0.83 0.84 0.83 0.84 0.84 0.86 0.86 0.79 0.7  0.5 ]\n",
            "Best Validation Alpha: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaoF4YWNmlR9",
        "colab_type": "code",
        "colab": {},
        "outputId": "68799550-af6a-4476-ebb1-fba38cdeeb9a"
      },
      "source": [
        "print(\"Training Acc: {}\".format(l1_acc2))\n",
        "# penalty at which validation accuracy is maximum\n",
        "max_index_2  = np.argmax(l1_acc2)\n",
        "best_alpha = alpha_vals[max_index_2]\n",
        "print(\"Best Training Alpha: {}\".format(best_alpha))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Acc: [1.   1.   1.   1.   1.   1.   0.97 0.92 0.85 0.73 0.52]\n",
            "Best Training Alpha: 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aV-RDzcmlR_",
        "colab_type": "code",
        "colab": {},
        "outputId": "50d14c04-f673-4328-ddef-3b420c7b707f"
      },
      "source": [
        "plt.plot(range(0,len(alpha_vals)), l1_acc, color='b', label='Average Validation Performance')\n",
        "plt.plot(range(0,len(alpha_vals)), l1_acc2, color='r', label='Average Training Performance')\n",
        "plt.plot((max_index_l1, max_index_l1), (0, l1_acc[max_index_l1]), ls='dotted', color='b')\n",
        "plt.plot((max_index_2, max_index_2), (0, l1_acc2[max_index_2]), ls='dotted', color='r')\n",
        "plt.xticks(range(0,len(alpha_vals)), alpha_vals, rotation='vertical') \n",
        "\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEbXNvJmlSJ",
        "colab_type": "text"
      },
      "source": [
        "As we look at the graph, the average training performance is higher than the Average validation performance. This is clear indication of overfitting of the model. Model is over fit for lower value of alpha and model is underfit for higher value of alpha. Average validation performance is highest at alpha value of 333 and that is the reason we choose the alpha value as 333."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaU8BjSfmlSJ",
        "colab_type": "text"
      },
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt64iSokmlSK",
        "colab_type": "code",
        "colab": {},
        "outputId": "81cd5b9e-0d4d-43a7-b221-9ea9bb2940be"
      },
      "source": [
        "alpha_val = 333\n",
        "multi_lr = LogisticRegression(C=1/alpha_val, penalty='l1', multi_class='ovr')\n",
        "multi_lr.fit(Mtrain.iloc[:,1:],Mtrain['label'])\n",
        "y_predict =multi_lr .predict(Mtest.iloc[:,1:])\n",
        "\n",
        "#Evaluate our model\n",
        "model_acc = accuracy_score(y_predict, Mtest['label'])\n",
        "print(\"Model Accuracy is: {}\".format(model_acc))\n",
        "\n",
        "print(\"Model Coeff: {}\".format(np.append(lr.intercept_, lr.coef_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy is: 0.8650793650793651\n",
            "Model Coeff: [ 16.22443839   0.           0.           0.           0.\n",
            "   0.           0.           0.          -2.82538694   0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.           0.           0.           0.           0.\n",
            "   0.          -7.5897989   -9.76581918 -16.58124943   0.\n",
            "  -4.81282941   0.           0.          -0.18348465   0.\n",
            "  -3.34246062]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DjQlhvrmlSM",
        "colab_type": "code",
        "colab": {},
        "outputId": "ea53244a-1e70-4cd7-85dd-2a8ff3278632"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_true = Mtest['label']\n",
        "print(classification_report(y_true, y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.94      0.93        68\n",
            "           1       0.86      0.97      0.92        72\n",
            "           2       0.94      0.84      0.88        86\n",
            "           3       0.90      0.85      0.87        78\n",
            "           4       0.81      0.85      0.83        72\n",
            "           5       0.90      0.80      0.85        75\n",
            "           6       0.87      0.92      0.90        66\n",
            "           7       0.94      0.89      0.91        83\n",
            "           8       0.77      0.81      0.79        85\n",
            "           9       0.77      0.80      0.79        71\n",
            "\n",
            "   micro avg       0.87      0.87      0.87       756\n",
            "   macro avg       0.87      0.87      0.87       756\n",
            "weighted avg       0.87      0.87      0.87       756\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5tsPypmlSR",
        "colab_type": "code",
        "colab": {},
        "outputId": "74f8bb93-e572-468e-c4ef-546e02d115f2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(Mtest['label'],y_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[64,  0,  0,  0,  0,  0,  1,  0,  3,  0],\n",
              "       [ 0, 70,  0,  0,  0,  0,  0,  0,  2,  0],\n",
              "       [ 0,  2, 72,  0,  2,  1,  3,  2,  3,  1],\n",
              "       [ 0,  1,  3, 66,  0,  3,  0,  0,  2,  3],\n",
              "       [ 0,  0,  0,  1, 61,  1,  1,  0,  3,  5],\n",
              "       [ 1,  0,  1,  4,  0, 60,  3,  0,  3,  3],\n",
              "       [ 2,  0,  0,  0,  2,  0, 61,  0,  1,  0],\n",
              "       [ 0,  2,  1,  0,  1,  0,  0, 74,  2,  3],\n",
              "       [ 1,  6,  0,  0,  2,  2,  1,  2, 69,  2],\n",
              "       [ 2,  0,  0,  2,  7,  0,  0,  1,  2, 57]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btPsGd3KmlSU",
        "colab_type": "text"
      },
      "source": [
        "The model shows signs of overfitting as we can see that the accuracy scores for the test data is lower than that of training data."
      ]
    }
  ]
}